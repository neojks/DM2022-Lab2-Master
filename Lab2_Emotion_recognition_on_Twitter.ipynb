{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process\n",
        "1.label of emotions->one-hot encoding\n",
        "\n",
        "2.text of each twitter->text to sequence transformation + pad the sequences to the same length.\n",
        "\n",
        "3.model:Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
        "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)  # layer norm\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
        "                                          output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "## Hyperparameters fot tokenizer\n",
        "vocab_size = 10000\n",
        "maxlen = 200  # Only consider the last 200 words of each movie review\n",
        "\n",
        "\n",
        "#  Using Functional API\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(8, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing-Data Formation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HXp_jvO74T4"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "data_id=[]\n",
        "with open('./data_identification.csv', newline='') as csvfile:\n",
        "\n",
        "\n",
        "  data = csv.reader(csvfile)\n",
        "  for i in data:\n",
        "    data_id.append(i)\n",
        "\n",
        "emo=[]\n",
        "with open('./emotion.csv', newline='') as csvfile:\n",
        "  data = csv.reader(csvfile)\n",
        "  for i in data:\n",
        "    emo.append(i)\n",
        "\n",
        "text=[]\n",
        "with open(\"./tweets_DM.json\") as file:\n",
        "    for line in file.readlines():\n",
        "      data=json.loads(line)\n",
        "      text.append(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0x3140b1', 'sadness']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del(emo[0])\n",
        "emo[0]\n",
        "#emo is equal to train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train size: 1455563\n",
            "test size: 411972\n",
            "0x29e452\n"
          ]
        }
      ],
      "source": [
        "del(data_id[0])\n",
        "train_data_id=[]\n",
        "test_data_id=[]\n",
        "for i,j in data_id:\n",
        "    if j=='train':\n",
        "        train_data_id.append(i)\n",
        "    else:\n",
        "        test_data_id.append(i)\n",
        "print(\"train size:\",len(train_data_id))\n",
        "print(\"test size:\",len(test_data_id))\n",
        "print(train_data_id[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'_score': 391,\n",
              " '_index': 'hashtag_tweets',\n",
              " '_source': {'tweet': {'hashtags': ['Snapchat'],\n",
              "   'tweet_id': '0x376b20',\n",
              "   'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}},\n",
              " '_crawldate': '2015-05-23 11:42:47',\n",
              " '_type': 'tweets'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': {'hashtags': ['Snapchat'],\n",
              "  'tweet_id': '0x376b20',\n",
              "  'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[0]['_source']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "emo_dict={}\n",
        "for tweet_id,emotion in emo:\n",
        "    emo_dict[tweet_id]=emotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Parse the text.According to the tweet_id of the text:\n",
        "#   if it is in train_data_id:\n",
        "#       match the emo and produce train_dataset(text+label(emo[1]))\n",
        "#   else:\n",
        "#       produce test_datasaet(text only)\n",
        "\n",
        "def match(tweet_id):\n",
        "    return emo_dict.get(tweet_id)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset=[]\n",
        "missing_data=[]\n",
        "test_dataset=[]\n",
        "for dict in text:\n",
        "    tweet_id=dict['_source']['tweet']['tweet_id']\n",
        "    score=dict['_score']\n",
        "    index=dict['_index']\n",
        "    hashtags=dict['_source']['tweet']['hashtags']\n",
        "    text=dict['_source']['tweet']['text']\n",
        "    if tweet_id  in train_data_id:\n",
        "        emotion=match(tweet_id)\n",
        "        if emotion!=None:\n",
        "            train_dataset.append([tweet_id,score,index,hashtags,text,emotion])\n",
        "        else:\n",
        "            missing_data.append([tweet_id,score,index,hashtags,text])\n",
        "    else:\n",
        "        test_dataset.append([tweet_id,score,index,hashtags,text])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "train_df=pd.DataFrame(train_dataset,columns=['tweet_id','score','index','hashtags','text','emotion'])\n",
        "test_df=pd.DataFrame(test_dataset,columns=['tweet_id','score','index','hashtags','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>score</th>\n",
              "      <th>index</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>text</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0x376b20</td>\n",
              "      <td>391</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[Snapchat]</td>\n",
              "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0x2d5350</td>\n",
              "      <td>433</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
              "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0x1cd5b0</td>\n",
              "      <td>376</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0x1d755c</td>\n",
              "      <td>120</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[authentic, LaughOutLoud]</td>\n",
              "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0x2c91a8</td>\n",
              "      <td>1021</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tweet_id  score           index                       hashtags  \\\n",
              "0  0x376b20    391  hashtag_tweets                     [Snapchat]   \n",
              "1  0x2d5350    433  hashtag_tweets  [freepress, TrumpLegacy, CNN]   \n",
              "2  0x1cd5b0    376  hashtag_tweets                             []   \n",
              "3  0x1d755c    120  hashtag_tweets      [authentic, LaughOutLoud]   \n",
              "4  0x2c91a8   1021  hashtag_tweets                             []   \n",
              "\n",
              "                                                text       emotion  \n",
              "0  People who post \"add me on #Snapchat\" must be ...  anticipation  \n",
              "1  @brianklaas As we see, Trump is dangerous to #...       sadness  \n",
              "2                Now ISSA is stalking Tasha 😂😂😂 <LH>          fear  \n",
              "3  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy  \n",
              "4       Still waiting on those supplies Liscus. <LH>  anticipation  "
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.to_csv(r'C:/Users/Neo/Desktop/DM Kaggle Competition/train_df.csv')\n",
        "test_df.to_csv(r'C:/Users/Neo/Desktop/DM Kaggle Competition/test_df.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "train_df = pd.read_csv('./train_df.csv')\n",
        "test_df = pd.read_csv('./test_df.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>score</th>\n",
              "      <th>index</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>text</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0x376b20</td>\n",
              "      <td>391</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['Snapchat']</td>\n",
              "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0x2d5350</td>\n",
              "      <td>433</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['freepress', 'TrumpLegacy', 'CNN']</td>\n",
              "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0x1cd5b0</td>\n",
              "      <td>376</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0x1d755c</td>\n",
              "      <td>120</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['authentic', 'LaughOutLoud']</td>\n",
              "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0x2c91a8</td>\n",
              "      <td>1021</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  tweet_id  score           index  \\\n",
              "0           0  0x376b20    391  hashtag_tweets   \n",
              "1           1  0x2d5350    433  hashtag_tweets   \n",
              "2           2  0x1cd5b0    376  hashtag_tweets   \n",
              "3           3  0x1d755c    120  hashtag_tweets   \n",
              "4           4  0x2c91a8   1021  hashtag_tweets   \n",
              "\n",
              "                              hashtags  \\\n",
              "0                         ['Snapchat']   \n",
              "1  ['freepress', 'TrumpLegacy', 'CNN']   \n",
              "2                                   []   \n",
              "3        ['authentic', 'LaughOutLoud']   \n",
              "4                                   []   \n",
              "\n",
              "                                                text       emotion  \n",
              "0  People who post \"add me on #Snapchat\" must be ...  anticipation  \n",
              "1  @brianklaas As we see, Trump is dangerous to #...       sadness  \n",
              "2                Now ISSA is stalking Tasha 😂😂😂 <LH>          fear  \n",
              "3  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy  \n",
              "4       Still waiting on those supplies Liscus. <LH>  anticipation  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>score</th>\n",
              "      <th>index</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0x28b412</td>\n",
              "      <td>232</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['bibleverse']</td>\n",
              "      <td>Confident of your obedience, I write to you, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0x2de201</td>\n",
              "      <td>989</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0x218443</td>\n",
              "      <td>66</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['materialism', 'money', 'possessions']</td>\n",
              "      <td>When do you have enough ? When are you satisfi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0x2939d5</td>\n",
              "      <td>104</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['GodsPlan', 'GodsWork']</td>\n",
              "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0x26289a</td>\n",
              "      <td>310</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  tweet_id  score           index  \\\n",
              "0           0  0x28b412    232  hashtag_tweets   \n",
              "1           1  0x2de201    989  hashtag_tweets   \n",
              "2           2  0x218443     66  hashtag_tweets   \n",
              "3           3  0x2939d5    104  hashtag_tweets   \n",
              "4           4  0x26289a    310  hashtag_tweets   \n",
              "\n",
              "                                  hashtags  \\\n",
              "0                           ['bibleverse']   \n",
              "1                                       []   \n",
              "2  ['materialism', 'money', 'possessions']   \n",
              "3                 ['GodsPlan', 'GodsWork']   \n",
              "4                                       []   \n",
              "\n",
              "                                                text  \n",
              "0  Confident of your obedience, I write to you, k...  \n",
              "1  \"Trust is not the same as faith. A friend is s...  \n",
              "2  When do you have enough ? When are you satisfi...  \n",
              "3  God woke you up, now chase the day #GodsPlan #...  \n",
              "4  In these tough times, who do YOU turn to as yo...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_id_to_tweet_id={}\n",
        "for id,tweet_id in zip(test_df['Unnamed: 0'],test_df['tweet_id']):\n",
        "    test_id_to_tweet_id[id]=tweet_id\n",
        "\n",
        "train_id_to_tweet_id={}\n",
        "for id,tweet_id in zip(train_df['Unnamed: 0'],train_df['tweet_id']):\n",
        "    train_id_to_tweet_id[id]=tweet_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>index</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>232</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['bibleverse']</td>\n",
              "      <td>Confident of your obedience, I write to you, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>989</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['materialism', 'money', 'possessions']</td>\n",
              "      <td>When do you have enough ? When are you satisfi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['GodsPlan', 'GodsWork']</td>\n",
              "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>310</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   score           index                                 hashtags  \\\n",
              "0    232  hashtag_tweets                           ['bibleverse']   \n",
              "1    989  hashtag_tweets                                       []   \n",
              "2     66  hashtag_tweets  ['materialism', 'money', 'possessions']   \n",
              "3    104  hashtag_tweets                 ['GodsPlan', 'GodsWork']   \n",
              "4    310  hashtag_tweets                                       []   \n",
              "\n",
              "                                                text  \n",
              "0  Confident of your obedience, I write to you, k...  \n",
              "1  \"Trust is not the same as faith. A friend is s...  \n",
              "2  When do you have enough ? When are you satisfi...  \n",
              "3  God woke you up, now chase the day #GodsPlan #...  \n",
              "4  In these tough times, who do YOU turn to as yo...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df=test_df.filter(items=['\tUnnamed: 0','score','index','hashtags','text'])\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>index</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>232</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['bibleverse']</td>\n",
              "      <td>Confident of your obedience, I write to you, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>989</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['materialism', 'money', 'possessions']</td>\n",
              "      <td>When do you have enough ? When are you satisfi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>['GodsPlan', 'GodsWork']</td>\n",
              "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>310</td>\n",
              "      <td>hashtag_tweets</td>\n",
              "      <td>[]</td>\n",
              "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   score           index                                 hashtags  \\\n",
              "0    232  hashtag_tweets                           ['bibleverse']   \n",
              "1    989  hashtag_tweets                                       []   \n",
              "2     66  hashtag_tweets  ['materialism', 'money', 'possessions']   \n",
              "3    104  hashtag_tweets                 ['GodsPlan', 'GodsWork']   \n",
              "4    310  hashtag_tweets                                       []   \n",
              "\n",
              "                                                text  \n",
              "0  Confident of your obedience, I write to you, k...  \n",
              "1  \"Trust is not the same as faith. A friend is s...  \n",
              "2  When do you have enough ? When are you satisfi...  \n",
              "3  God woke you up, now chase the day #GodsPlan #...  \n",
              "4  In these tough times, who do YOU turn to as yo...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train_label=train_df.iloc[:,-1]\n",
        "train_df=train_df.filter(items=['\tUnnamed: 0','score','index','hashtags','text'])\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0          anticipation\n",
              "1               sadness\n",
              "2                  fear\n",
              "3                   joy\n",
              "4          anticipation\n",
              "               ...     \n",
              "1455558             joy\n",
              "1455559             joy\n",
              "1455560             joy\n",
              "1455561             joy\n",
              "1455562             joy\n",
              "Name: emotion, Length: 1455563, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing-Attribute Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anticipation sadness fear joy anger trust disgust surprise\n"
          ]
        }
      ],
      "source": [
        "print(*y_train_label.unique())\n",
        "y_label_encoding={\n",
        "    'anticipation':0, 'sadness':1, 'fear':2, 'joy':3, 'anger':4, 'trust':5,\n",
        "       'disgust':6, 'surprise':7\n",
        "}\n",
        "y_label_decoding={\n",
        "    0:'anticipation', 1:'sadness', 2:'fear', 3:'joy', 4:'anger', 5:'trust',\n",
        "       6:'disgust', 7:'surprise'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    0\n",
            "1    1\n",
            "2    2\n",
            "3    3\n",
            "4    0\n",
            "Name: emotion, dtype: object\n",
            "{'anticipation': 248935, 'sadness': 193437, 'fear': 63999, 'joy': 516017, 'anger': 39867, 'trust': 205478, 'disgust': 139101, 'surprise': 48729}\n"
          ]
        }
      ],
      "source": [
        "#Encode the emotion and Count the distributioon\n",
        "y_label_count={\n",
        "    'anticipation':0, 'sadness':0, 'fear':0, 'joy':0, 'anger':0, 'trust':0,\n",
        "       'disgust':0, 'surprise':0\n",
        "}\n",
        "for i,j in enumerate(y_train_label):\n",
        "    y_label_count[j]+=1\n",
        "    y_train_label[i]=y_label_encoding[j]\n",
        "\n",
        "print(y_train_label.head())\n",
        "print(y_label_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization\n",
        "->It is obvious that the label is imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 8 artists>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAI/CAYAAAAspk44AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhBklEQVR4nO3de5Skd13n8c/XDJfIJQEy5mACDotRN+BRyQhhvSwXDYGoyVlR4KgEZMlhAREvq3H1CIK4UTyLywrsxiWbBFBEEIkQiNkIgmggEy4J4SIjhE0ikJCEIHIz8Ns/6jdQGbp7OpOZ6W86r9c5fabq9zz1PL+qqa5691NV3TXGCAAAvXzDRk8AAICvJ9IAABoSaQAADYk0AICGRBoAQEMiDQCgoS0bPYF97bDDDhvbtm3b6GkAAOzRxRdf/KkxxtaVlm26SNu2bVt27Nix0dMAANijqvrYasu83AkA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKChLRs9AeDWa9upb9joKexTl592wkZPAeCrHEkDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaWlekVdXlVXVpVb2nqnbMsbtX1flV9eH5793meFXVC6tqZ1VdUlUPWNrOyXP9D1fVyUvjx8zt75yXrbX2AQCw2d2cI2kPHWN89xhj+zx/apILxhhHJblgnk+SRyY5an6dkuQlySK4kjwryYOSPDDJs5ai6yVJnrx0ueP3sA8AgE3tlrzceWKSs+bps5KctDR+9li4MMmhVXXPJI9Icv4Y47oxxvVJzk9y/Fx21zHGhWOMkeTs3ba10j4AADa19UbaSPJXVXVxVZ0yxw4fY3x8nv5EksPn6SOSXLF02Svn2FrjV64wvtY+AAA2tS3rXO/7xxhXVdU3JTm/qj64vHCMMapq7PvprW8fMxxPSZJ73/ve+3MaAAAHxLqOpI0xrpr/Xp3ktVm8p+yT86XKzH+vnqtfleReSxc/co6tNX7kCuNZYx+7z+/0Mcb2Mcb2rVu3rucqAQC0tsdIq6o7VdVddp1OclyS9yU5J8muT2ienOR18/Q5SR4/P+V5bJIb5kuW5yU5rqruNj8wcFyS8+ayz1TVsfNTnY/fbVsr7QMAYFNbz8udhyd57fytGFuS/PEY401VdVGSV1XVk5J8LMlPzvXPTfKoJDuTfC7JE5NkjHFdVT03yUVzveeMMa6bp5+a5MwkByd54/xKktNW2QcAwKa2x0gbY3wkyXetMH5tkoevMD6SPG2VbZ2R5IwVxnckuf969wEAsNn5iwMAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBD6460qjqoqt5dVa+f5+9TVe+oqp1V9adVdfs5fod5fudcvm1pG782xz9UVY9YGj9+ju2sqlOXxlfcBwDAZndzjqT9fJIPLJ3/3SQvGGN8a5Lrkzxpjj8pyfVz/AVzvVTV0Ukem+R+SY5P8uIZfgcleVGSRyY5Osnj5rpr7QMAYFNbV6RV1ZFJTkjyv+f5SvKwJK+eq5yV5KR5+sR5PnP5w+f6JyZ55Rjji2OMjybZmeSB82vnGOMjY4wvJXllkhP3sA8AgE1tvUfS/iDJryT5yjx/jySfHmPcOM9fmeSIefqIJFckyVx+w1z/q+O7XWa18bX2AQCwqe0x0qrqR5JcPca4+ADMZ69U1SlVtaOqdlxzzTUbPR0AgFtsPUfSvi/Jj1XV5Vm8FPmwJP89yaFVtWWuc2SSq+bpq5LcK0nm8kOSXLs8vttlVhu/do193MQY4/QxxvYxxvatW7eu4yoBAPS2x0gbY/zaGOPIMca2LN74/9djjJ9K8uYkj56rnZzkdfP0OfN85vK/HmOMOf7Y+enP+yQ5Ksk7k1yU5Kj5Sc7bz32cMy+z2j4AADa1W/J70n41yS9W1c4s3j/20jn+0iT3mOO/mOTUJBljXJbkVUnen+RNSZ42xvjyfM/Z05Ocl8WnR181111rHwAAm9qWPa/yNWOMtyR5yzz9kSw+mbn7Ol9I8hOrXP55SZ63wvi5Sc5dYXzFfQAAbHb+4gAAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaGiPkVZVd6yqd1bVe6vqsqr6rTl+n6p6R1XtrKo/rarbz/E7zPM75/JtS9v6tTn+oap6xNL48XNsZ1WdujS+4j4AADa79RxJ+2KSh40xvivJdyc5vqqOTfK7SV4wxvjWJNcnedJc/0lJrp/jL5jrpaqOTvLYJPdLcnySF1fVQVV1UJIXJXlkkqOTPG6umzX2AQCwqe0x0sbCZ+fZ282vkeRhSV49x89KctI8feI8n7n84VVVc/yVY4wvjjE+mmRnkgfOr51jjI+MMb6U5JVJTpyXWW0fAACb2rrekzaPeL0nydVJzk/yj0k+Pca4ca5yZZIj5ukjklyRJHP5DUnusTy+22VWG7/HGvsAANjU1hVpY4wvjzG+O8mRWRz5+o79Oambq6pOqaodVbXjmmuu2ejpAADcYjfr051jjE8neXOSByc5tKq2zEVHJrlqnr4qyb2SZC4/JMm1y+O7XWa18WvX2Mfu8zp9jLF9jLF969atN+cqAQC0tJ5Pd26tqkPn6YOT/HCSD2QRa4+eq52c5HXz9DnzfObyvx5jjDn+2Pnpz/skOSrJO5NclOSo+UnO22fx4YJz5mVW2wcAwKa2Zc+r5J5JzpqfwvyGJK8aY7y+qt6f5JVV9dtJ3p3kpXP9lyZ5WVXtTHJdFtGVMcZlVfWqJO9PcmOSp40xvpwkVfX0JOclOSjJGWOMy+a2fnWVfQAAbGp7jLQxxiVJvmeF8Y9k8f603ce/kOQnVtnW85I8b4Xxc5Ocu959AABsdv7iAABAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGtqy0RO4Ndp26hs2egr71OWnnbDRUwAAduNIGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0NCWjZ4AALdu2059w0ZPYZ+6/LQTNnoKkGQdR9Kq6l5V9eaqen9VXVZVPz/H715V51fVh+e/d5vjVVUvrKqdVXVJVT1gaVsnz/U/XFUnL40fU1WXzsu8sKpqrX0AAGx263m588YkvzTGODrJsUmeVlVHJzk1yQVjjKOSXDDPJ8kjkxw1v05J8pJkEVxJnpXkQUkemORZS9H1kiRPXrrc8XN8tX0AAGxqe4y0McbHxxjvmqf/OckHkhyR5MQkZ83Vzkpy0jx9YpKzx8KFSQ6tqnsmeUSS88cY140xrk9yfpLj57K7jjEuHGOMJGfvtq2V9gEAsKndrA8OVNW2JN+T5B1JDh9jfHwu+kSSw+fpI5JcsXSxK+fYWuNXrjCeNfYBALCprTvSqurOSV6T5JljjM8sL5tHwMY+nttNrLWPqjqlqnZU1Y5rrrlmf04DAOCAWFekVdXtsgi0V4wx/nwOf3K+VJn579Vz/Kok91q6+JFzbK3xI1cYX2sfNzHGOH2MsX2MsX3r1q3ruUoAAK2t59OdleSlST4wxvhvS4vOSbLrE5onJ3nd0vjj56c8j01yw3zJ8rwkx1XV3eYHBo5Lct5c9pmqOnbu6/G7bWulfQAAbGrr+T1p35fkZ5JcWlXvmWP/JclpSV5VVU9K8rEkPzmXnZvkUUl2JvlckicmyRjjuqp6bpKL5nrPGWNcN08/NcmZSQ5O8sb5lTX2AQCwqe0x0sYYf5ukVln88BXWH0metsq2zkhyxgrjO5Lcf4Xxa1faBwDAZufPQgEANCTSAAAa8rc72Sv+Vh8A7F+OpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDWzZ6AgBwa7ft1Dds9BT2qctPO2Gjp0AcSQMAaEmkAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEMiDQCgIZEGANCQSAMAaEikAQA0tMdIq6ozqurqqnrf0tjdq+r8qvrw/Pduc7yq6oVVtbOqLqmqByxd5uS5/oer6uSl8WOq6tJ5mRdWVa21DwCA24L1HEk7M8nxu42dmuSCMcZRSS6Y55PkkUmOml+nJHlJsgiuJM9K8qAkD0zyrKXoekmSJy9d7vg97AMAYNPbY6SNMd6a5Lrdhk9MctY8fVaSk5bGzx4LFyY5tKrumeQRSc4fY1w3xrg+yflJjp/L7jrGuHCMMZKcvdu2VtoHAMCmt7fvSTt8jPHxefoTSQ6fp49IcsXSelfOsbXGr1xhfK19AABserf4gwPzCNjYB3PZ631U1SlVtaOqdlxzzTX7cyoAAAfE3kbaJ+dLlZn/Xj3Hr0pyr6X1jpxja40fucL4Wvv4OmOM08cY28cY27du3bqXVwkAoI+9jbRzkuz6hObJSV63NP74+SnPY5PcMF+yPC/JcVV1t/mBgeOSnDeXfaaqjp2f6nz8bttaaR8AAJvelj2tUFV/kuQhSQ6rqiuz+JTmaUleVVVPSvKxJD85Vz83yaOS7EzyuSRPTJIxxnVV9dwkF831njPG2PVhhKdm8QnSg5O8cX5ljX0AAGx6e4y0McbjVln08BXWHUmetsp2zkhyxgrjO5Lcf4Xxa1faBwDAbYG/OAAA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKChLRs9Abi12nbqGzZ6CvvU5aedsNFTAGCJI2kAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA2JNACAhkQaAEBDIg0AoCGRBgDQkEgDAGhoy0ZPAAC49dt26hs2egr71OWnnbDRU3AkDQCgI5EGANCQSAMAaEikAQA0JNIAABoSaQAADYk0AICGRBoAQEN+mS3ALeAXeAL7iyNpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQyINAKAhkQYA0JBIAwBoSKQBADQk0gAAGhJpAAANiTQAgIZEGgBAQ+0jraqOr6oPVdXOqjp1o+cDAHAgtI60qjooyYuSPDLJ0UkeV1VHb+ysAAD2v9aRluSBSXaOMT4yxvhSklcmOXGD5wQAsN91j7QjklyxdP7KOQYAsKnVGGOj57Cqqnp0kuPHGP9xnv+ZJA8aYzx9t/VOSXLKPPvtST50QCe6/xyW5FMbPYkN5jZwGyRug8RtkLgNErdBsvlug28ZY2xdacGWAz2Tm+mqJPdaOn/kHLuJMcbpSU4/UJM6UKpqxxhj+0bPYyO5DdwGidsgcRskboPEbZDctm6D7i93XpTkqKq6T1XdPsljk5yzwXMCANjvWh9JG2PcWFVPT3JekoOSnDHGuGyDpwUAsN+1jrQkGWOcm+TcjZ7HBtl0L+HuBbeB2yBxGyRug8RtkLgNktvQbdD6gwMAALdV3d+TBgBwmyTS9pOqOmn5ryNU1XOq6ofWWH97Vb1wL/d1aFU9den8N1fVq/dmWxulqrZV1fs2eh4boaqeUVUfqKpXbPRcOqiqv9voOXDg7P74dQu39ZCq+nf7YlsHQlU9u6p+eU/PD/twfzd5XtqMqurcqjp0o+exr3i5cz+pqjOTvH6Msd9jqaq2zX3df3/va3/ZDNdhb1XVB5P80BjjyluwjS1jjBv34bTYBKqqsnic/8pGz2U1q33v7819uqqeneSzY4zf33cz3H8O9HwP5PPSvrLe+8Gt4b6+NxxJuxmq6i+q6uKqumz+At1U1Wer6nlV9d6qurCqDp8/yf1YkudX1Xuq6r5Vdeb85bypqu+tqr+bl3lnVd1l/gT4+rn82VX1sqr6+6r6cFU9eY7fuaouqKp3VdWlVbXrT2SdluS+c1/PXz4qVVV3rKr/M9d/d1U9dI4/oar+vKreNPfxe/voNrpTVb1hXrf3VdVjquo3q+qief70+c2UqjpmrvfeJE9b2saqc6uq4+bt8q6q+rOquvMcP62q3l9Vl1TV78+xn5j7fG9VvXVfXL99rar+Z5J/k+SNVfXrVXXGvE+8e9f/7/z/fNu8zu/adaRg3mfeVlXnJHn/Bl6NfWp+T9W8L79v3ncfM5edXVUnLa37iqXvg1uN9T6WzPH7zvOXVtVvV9Vnl7bzn+f31iVV9VtzbFtVfaiqzk7yvtz0d012tPz4ddHyfbp2O8Jei6NOz56nn7H0Pf/KWsTeU5L8wtzWD2zEldmT+X3+D1X1t1n88vXUTZ8fVnosW/E+UEvPG/P8H1bVE1baTq3wvHSAr/dKzw2XV9Vhc/n2qnrLPL3rOfDtSV42nxNeV1Vvmc8Jz5rrfd19fdc2V9rfvMwxVfU38/vvvKq654G8HW62MYavdX4lufv89+As7hD3SDKS/Ogc/70kvzFPn5nk0UuXPTPJo5PcPslHknzvHL9rFp+yfUgWP+EkybOTvHfu57As/jTWN8/17jrXOSzJziSVZFuS9y3t66vnk/xSFr+6JEm+I8n/S3LHJE+Y8zhknv9Yknvtg9vox5P80dL5Q3bdbvP8y5Zur0uS/OA8/fylOa84t3md35rkTnO9X03ym/P/4UP52pHhQ+e/lyY5Ynms41eSy+d1+50kP71rvkn+IcmdknxjkjvO8aOS7JinH5LkX5LcZ6Ovwz6+PT4770fnZ/Grdw6f99t7Jvn3Sf5i6b710SRbNnrOe3Edb85jyeuTPG6efkoWR16S5LgsPuVWWfzA/fokPzi//7+S5NiNvp7rvC2WH69ucp/O1z+2/XKSZ8/T/5TkDvP0ofPfZyf55Y2+Tmtc12Pm49I3ZvHYv3NepzOzeH5Y7bFstfvAQzKfN+b5P8zi8XO17ZyZpeelA3zdV3puuDzJYfP89iRvWfp/vDjJwfP8E5J8fF6vXd8z21e6r+drj6cr7e92Sf4uydY59pjM58euX46k3TzPqMVRnwuziIajknwpi2+gZHGn2raHbXx7ko+PMS5KkjHGZ8bKh3JfN8b4/BjjU0nenMUfm68kv1NVlyT5v1n8HdPD97C/70/y8rmvD2YRPN82l10wxrhhjPGFLI7EfMsetrUelyb54ar63ar6gTHGDUkeWlXvqKpLkzwsyf1q8Z6BQ8cYu45wvWy37aw0t2OTHJ3k7VX1niQnz/EbknwhyUur6j8k+dzcxtuTnFmLI5EH7YPrtr8dl+TUed3ekkWg3juLB5Y/mrffn2VxG+zyzjHGRw/wPA+E70/yJ2OML48xPpnkb7L4weZvsvgF11uTPC7Ja1b5/unu5jyWPDiL//ck+eOlbRw3v96d5F1Z/BB21Fz2sTHGhftr8vvZeu/TlyR5RVX9dJJby33gB5K8dozxuTHGZ/L1v5x9tcey1e4Dq1ltOxtppeeGtZwzxvj80vnzxxjXzrE/z+IxIln9vr7S/r49yf2TnD8fZ38ji79k1Fb735PWRVU9JMkPJXnwGONz87DsHZP865hJnuTL2Xe36e5vFhxJfirJ1iTHjDH+taoun3PYW19cOr1P5j7G+IeqekCSRyX57aq6IIuXMrePMa6YL1WsZ84rza2y+EZ93O4rV9UDkzw8i59Gn57kYWOMp1TVg5KckOTiqjpmjHHtLbh6+1sl+fExxk3+9uy8zT6Z5LuyOGLyhaXF/3LAZtfH2Ul+Oou/QPLEDZ7LzbYPH0sqyX8dY/yv3ba/Lbfu+8Xy3G/MTd+Ws/zYcUIWRw5/NMmvV9V3HoC57Vdj8Qvcv+6xbI2LrHj77MV29rtVnhuW57/788Lu9+GVnhNXWm+t/b02yWVjjAfv5dU44BxJW79Dklw/H1S/I4ujOmv55yR3WWH8Q0nuWVXfmyS1eD/aSg/GJ9bi/WT3yOKQ9kVzDlfPQHtovnbka7V9Jcnbsoi7VNW3ZXFkZr/9Afqq+uYknxtjvDyLlzAfMBd9qhbvH3t0kowxPp3k01W166ehn1rH5i9M8n1V9a1zX3eqqm+b2z1kLH7x8S9kETOpqvuOMd4xxvjNJNek/3tzzkvyc1Vffc/e98zxQ7I4+vqVJD+TW8dRwVvqbUkeU1UHzaNmP5jknXPZmUmemSRjjFvje/Fu7mPJhVm8dJMswnSX85L8bH3tfZlHVNU37fPZ7n9rPX59Msk3VdU9quoOSX4kSarqG7J4e8abs3jbwyFJ7ryHbXXw1iQnVdXBVXWXLALzq1Z7LMvq94GPJTm6qu4wX514+B62s2G3zyrPDZdn8RJw8rXrt5ofrqq7V9XBSU7K4pWSm7u/DyXZWlUPnuvcrqrut3fX6MBwJG393pTkKVX1gSz+o/f0UsIrs3iJ6hmZYZIkY4wvzTcw/o95Z/t8Fj9V7+6SLF7mPCzJc8cY/1SLX9Hwl/Nlrx1JPji3eW1Vvb0Wb7B9Y5IXLW3nxUleMi9zY5InjDG+ODtgf/jOLN6Y+pUk/5rkP2XxDfW+JJ/IIjZ3eWKSM6pqJPmrPW14jHFNLd4U+yfzATtZHK7+5ySvq6o7ZnF04RfnsudX1VFz7IIs3ufX2XOT/EGSS+aT0EezeFJ6cZLXVNXjs7gf3pqPkqzHyOIn3gdn8X82kvzKGOMTSTLG+OT8PvyLDZvhLXNzH0uemeTlVfXr87I3JMkY46+q6t8m+fv5/fzZLI4wfnk/zXu/2O3x6/NZhNmuZf9aVc/JItCvynzMy+IHlZdX1SFZfH+/cIzx6ar6yySvrsWHSX5ujPG2A3pl9mCM8a6q+tMs7tdX56aPh8kioFZ6LHtmVr4PXFFVr8ri8fWjWbz0vdZ2bvK8NMb4x31/LVe10nPDwVm8JPvcLN7isZZ3JnlNFi9PvnyMsWMeNV73/ubz76OTvHDed7Zk8Zjb9s9N+hUcDdWt7GPksK/MI8fvGmOs+v7IqvrGLN5v8oB1vK/lVm9e38+PMUZVPTaLN5Df6j7Ryt67rd8H5g/n28cYT9/ouRxojqQBLcyXJ96SZNUfTmrxCz9fmuQFt4VAm45J8ofzZfBPJ/nZjZ0OG8B94DbKkTQAgIZ8cAAAoCGRBgDQkEgDAGhIpAEANCTSAAAaEmkAAA39fxXM8G7FznHkAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.bar(y_label_count.keys(),y_label_count.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text to sequence transformation and one-hot encoding of emotion "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train=train_df['text']\n",
        "x_test=test_df['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "## text to sequences\n",
        "texts_to_int = tokenizer.texts_to_sequences(x_train)\n",
        "## pad sequences\n",
        "texts_to_int_pad = keras.preprocessing.sequence.pad_sequences(texts_to_int,\n",
        "                                                              maxlen=maxlen,\n",
        "                                                              truncating='pre',\n",
        "                                                              padding='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  132,  220,    1],\n",
              "       [   0,    0,    0, ...,    1,    1,  610],\n",
              "       [   0,    0,    0, ..., 2493,  892,    1],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,   49,  828,    1],\n",
              "       [   0,    0,    0, ...,   16, 4296,   15],\n",
              "       [   0,    0,    0, ...,   18,  424,    1]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts_to_int_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "## X and y for train and val\n",
        "x_train = texts_to_int_pad\n",
        "y_train = y_train_label\n",
        "\n",
        "texts_to_int2=tokenizer.texts_to_sequences(x_test)\n",
        "texts_to_int_pad2 = keras.preprocessing.sequence.pad_sequences(texts_to_int2,\n",
        "                                                              maxlen=maxlen,\n",
        "                                                              truncating='pre',\n",
        "                                                              padding='pre')\n",
        "x_test=texts_to_int_pad2                                                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1455563, 200)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(411972, 200)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train_label, test_size=0.25, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encoder(y):\n",
        "    output=[]\n",
        "    for yy in y:\n",
        "        if yy==0:\n",
        "            output.append([1,0,0,0,0,0,0,0])\n",
        "        elif yy==1:\n",
        "            output.append([0,1,0,0,0,0,0,0])\n",
        "        elif yy==2:\n",
        "            output.append([0,0,1,0,0,0,0,0])\n",
        "        elif yy==3:\n",
        "            output.append([0,0,0,1,0,0,0,0])\n",
        "        elif yy==4:\n",
        "            output.append([0,0,0,0,1,0,0,0])\n",
        "        elif yy==5:\n",
        "            output.append([0,0,0,0,0,1,0,0])\n",
        "        elif yy==6:\n",
        "            output.append([0,0,0,0,0,0,1,0])\n",
        "        elif yy==7:\n",
        "            output.append([0,0,0,0,0,0,0,1])\n",
        "    return np.array(output)\n",
        "\n",
        "y_train=one_hot_encoder(y_train)\n",
        "y_val=one_hot_encoder(y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ntrain_tensor= tf.data.Dataset.from_tensor_slices((x_train,y_train))\\nval_tensor= tf.data.Dataset.from_tensor_slices((x_val,y_val))\\ntest_tensor= tf.data.Dataset.from_tensor_slices(x_test)\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#type transformation to avoid error\n",
        "x_train = np.asarray(x_train).astype('float32')\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "x_val = np.asarray(x_val).astype('float32')\n",
        "y_val = np.asarray(y_val).astype('float32')\n",
        "x_test = np.asarray(x_test).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1091672, 200)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1091672, 8)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training\n",
        "->I only use twitter text and emotion label to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8529/8529 [==============================] - 1167s 137ms/step - loss: 0.2578 - accuracy: 0.5274 - val_loss: 0.2439 - val_accuracy: 0.5491\n",
            "Epoch 2/10\n",
            "8529/8529 [==============================] - 1089s 128ms/step - loss: 0.2420 - accuracy: 0.5530 - val_loss: 0.2385 - val_accuracy: 0.5558\n",
            "Epoch 3/10\n",
            "8529/8529 [==============================] - 1070s 125ms/step - loss: 0.2361 - accuracy: 0.5636 - val_loss: 0.2355 - val_accuracy: 0.5625\n",
            "Epoch 4/10\n",
            "8529/8529 [==============================] - 3161s 371ms/step - loss: 0.2326 - accuracy: 0.5711 - val_loss: 0.2339 - val_accuracy: 0.5655\n",
            "Epoch 5/10\n",
            "8529/8529 [==============================] - 1197s 140ms/step - loss: 0.2300 - accuracy: 0.5764 - val_loss: 0.2331 - val_accuracy: 0.5680\n",
            "Epoch 6/10\n",
            "8529/8529 [==============================] - 1208s 142ms/step - loss: 0.2278 - accuracy: 0.5811 - val_loss: 0.2332 - val_accuracy: 0.5684\n",
            "Epoch 7/10\n",
            "8529/8529 [==============================] - 1209s 142ms/step - loss: 0.2261 - accuracy: 0.5852 - val_loss: 0.2329 - val_accuracy: 0.5698\n",
            "Epoch 8/10\n",
            "8529/8529 [==============================] - 1213s 142ms/step - loss: 0.2245 - accuracy: 0.5888 - val_loss: 0.2341 - val_accuracy: 0.5690\n",
            "Epoch 9/10\n",
            "8529/8529 [==============================] - 1205s 141ms/step - loss: 0.2229 - accuracy: 0.5927 - val_loss: 0.2336 - val_accuracy: 0.5693\n",
            "Epoch 10/10\n",
            "8529/8529 [==============================] - 1152s 135ms/step - loss: 0.2216 - accuracy: 0.5952 - val_loss: 0.2341 - val_accuracy: 0.5680\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save the weights\n",
        "import os\n",
        "CKP_DIR_SAVE_WEIGHTS = './checkpoints_save_weights'\n",
        "model.save_weights(os.path.join(CKP_DIR_SAVE_WEIGHTS, f'ckpt-{10}'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1abeff0c198>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#load the weights\n",
        "model.load_weights('checkpoints_save_weights/ckpt-10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_max(pred_dis):\n",
        "    max_id=-1\n",
        "    max=0\n",
        "    for id,prob in enumerate(pred_dis):\n",
        "        if prob>max:\n",
        "            max_id=id\n",
        "            max=prob\n",
        "    return max_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_pred=model.predict(x_test)\n",
        "emotion=[]\n",
        "tweet_id=[]\n",
        "for test_id,pred_dis in enumerate(y_test_pred):\n",
        "    tweet_id.append(test_id_to_tweet_id[test_id])\n",
        "    emotion.append(y_label_decoding[find_max(pred_dis)])\n",
        "\n",
        "output={'id':tweet_id,'emotion':emotion}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "output=pd.DataFrame(data=output)\n",
        "output.set_index(\"id\" , inplace=True)\n",
        "\n",
        "output.to_csv(r'C:/Users/Neo/Desktop/DM Kaggle Competition/output_submission.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.13 ('neo')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "352e7bb3af4b2b633cc9de5a80c8e118f1d45af0f7ca09c6928580a8ee7d7335"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
